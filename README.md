<div align="center">

![Cover Page](images/Resized.png)

# ğŸ¤– **Inside-Medium : The Right Article, at the Right Time**

*Discover trending, relevant reads instantly with AI-powered article matching!*

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

[Features](#features) â€¢ [Installation](#installation) â€¢ [Documentation](#documentation) â€¢ [Usage](#usage) â€¢ [Contributing](#contributing)

</div>

---

## ğŸŒŸ Overview

**Inside-Medium** is an AI-powered content recommendation engine designed to help readers find the most relevant and high-quality Medium articles based on their interests or selected articles. By leveraging Natural Language Processing (NLP) and Topic Modeling (NMF) techniques, the system extracts hidden topics from articles, encodes them into meaningful vectors, and uses cosine similarity to recommend similar content.

---

## ğŸ“š Dataset - Medium Articles Dataset

ğŸ“ **Source**: [Medium Articles Dataset â€“ Kaggle](https://www.kaggle.com/datasets/dorianlazar/medium-articles-dataset/data)

The **Medium Articles Dataset** is a curated collection of publicly available articles published on Medium.com. It contains both **textual content and engagement metadata**, making it ideal for tasks like recommendation systems, NLP, and content analysis.

#### ğŸ“ Dataset Highlights:

* **Total Records**: \~8,000 articles
* **Key Columns**:

  * `title`: Title of the article
  * `subtitle`: Subtitle or secondary heading
  * `author`: Author of the article
  * `date`: Publication date
  * `claps`: Number of claps (engagement metric)
  * `reading_time`: Estimated reading time (in minutes)
  * `publication`: Name of the publication (if any)
  * `url`: Link to the original article
  * `article`: Full textual content of the article

#### âœ… Why This Dataset?

* Great for **topic modeling**, **text classification**, and **recommendation systems**
* Contains real-world engagement signals (`claps`) to enrich the model
* Useful for building **AI-driven content discovery platforms** like Inside-Medium

> ğŸ“Œ **Dataset Link**: [https://www.kaggle.com/datasets/dorianlazar/medium-articles-dataset/data](https://www.kaggle.com/datasets/dorianlazar/medium-articles-dataset/data)

---

## ğŸš€ Features of *Inside-Medium*

* ğŸ” **Content-Based Article Recommendation**
  Recommends articles similar to a userâ€™s query based on textual content and latent topic features.


* ğŸ“ˆ **Similarity Scoring**
  Calculates cosine similarity between articles to identify the most relevant ones.

* ğŸ“‘ **Interactive Query Support**
  Users can input any article title to retrieve a list of the most similar articles.

* ğŸ§¼ **Modular, Clean Codebase**
  Structured using classes for vectorization, normalization, and similarity search with full docstrings and logging.

* ğŸ“¦ **Reproducible Pipeline**
  Complete workflow from raw data to recommendationsâ€”easy to extend or integrate into other systems.

* ğŸ§¾ **Logging and Error Handling**
  Built-in logging for debugging and tracking progress/errors in each module.

* ğŸ“‚ **Scalable Design**
  Easy to adapt for larger datasets or additional features like user profiling or collaborative filtering.

---

## ğŸ› ï¸ Installation

#### Step - 1: Repository Cloning

```bash
# Clone the repository
git clone https://github.com/priyam-hub/Inside-Medium.git

# Navigate into the directory
cd Inside-Medium
```

#### Step - 2: Enviornmental Setup and Dependency Installation

```bash
# Run env_setup.sh
bash env_setup.sh

# Select 1 to create Python Environment
# Select 2 to create Conda Environment

# Python Version - 3.10

# Make the Project to run as a Local Package
python setup.py
```

#### Step - 3: Creation of Kaggle API

- Log-In to your Kaggle Account
- An API token downloaded from Kaggle Account Settings â†’ Create New Token.
- Manually place your kaggle.json (downloaded from https://www.kaggle.com/account) into this location:

```plaintext
C:\Users\<Your_Username>\.kaggle\kaggle.json
```

#### Step - 4: Create a .env file in the root directory to add Credentials or (Change the filename ".sample_env" to ".env")

```bash
KAGGLE_USERNAME = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
KAGGLE_API_KEY  = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```

#### Step - 5: Run the Full Pipeline

```bash
# Run the Main Python Script
python main.py
```

#### Step - 6: Run the Flask Server (Up-Coming)

```bash
# Run the Web App using Flask Server
python web/app.py
```

**Note** - Upon running, navigate to the provided local URL in your browser to interact with the Inside-Medium Recommendation Engine

---

## ğŸ§° Technology Stack

**Python** â€“ Core programming language used to build the recommendation pipeline, data processing, and backend logic.
ğŸ”— [Install Python](https://www.python.org/downloads/)

**Pandas & NumPy** â€“ Used for efficient data manipulation, cleaning, and numerical operations.
ğŸ”— [Pandas Documentation](https://pandas.pydata.org/docs/) | [NumPy Documentation](https://numpy.org/doc/)

**Scikit-learn** â€“ Used for feature extraction (TF-IDF), dimensionality reduction (NMF), and similarity computation.
ğŸ”— [Scikit-learn Documentation](https://scikit-learn.org/stable/)

**Flask** â€“ Lightweight Python web framework used to serve the recommendation engine as an API or simple web app.
ğŸ”— [Flask Installation](https://flask.palletsprojects.com/en/latest/installation/)

**Logging** â€“ Pythonâ€™s built-in `logging` module used for tracking system operations and debugging.
ğŸ”— [Logging Documentation](https://docs.python.org/3/library/logging.html)

**Kaggle API** â€“ Used to automatically fetch and manage the Medium Articles dataset.
ğŸ”— [Kaggle API Setup Guide](https://github.com/Kaggle/kaggle-api)

---

## ğŸ“ Project Structure

```plaintext
Inside-Medium/
â”œâ”€â”€ .env                                      # Store the Kaggle Username and API Key
â”œâ”€â”€ .gitignore                                # Ignoring files for Git
â”œâ”€â”€ env_setup.sh                              # Package installation configuration
â”œâ”€â”€ folder_structure.py                       # Contains the Project Folder Structure
â”œâ”€â”€ LICENCE                                   # MIT License
â”œâ”€â”€ main.py                                   # Full Pipeline of the Project
â”œâ”€â”€ README.md                                 # Project documentation
â”œâ”€â”€ requirements.txt                          # Python dependencies
â”œâ”€â”€ setup.py                                  # Create the Project as Python Package
â”œâ”€â”€ config/                                   # Configuration files
â”‚   â”œâ”€â”€ __init__.py                           
â”‚   â””â”€â”€ config.py/                            # All Configuration Variables of Pipeline
â”œâ”€â”€ data/                                     # Data Directory
â”‚   â”œâ”€â”€ images/                               # Medium Article Images Directory
â”‚   â”œâ”€â”€ medium_normalized_data.csv            # Normalized Data of the Medium Articles
â”‚   â”œâ”€â”€ medium_processed_data.csv             # Processed Data of the Medium Articles
â”‚   â””â”€â”€ medium_raw_data.csv                   # Raw Data of the Medium Articles
â”œâ”€â”€ logger/                                   # Logger Setup Directory
â”‚   â””â”€â”€ logger.py                             # Format of the Logger Setup of the Project
â”œâ”€â”€ notebooks/                                # Jupyter notebooks for experimentation
â”‚   â””â”€â”€ Recommendation_System.ipynb           # Experimented Recommendation Engine in Jupyter Notebook
â”œâ”€â”€ results/                                  # Directory to Store the results of the Project
â”‚   â””â”€â”€ eda_results/                          # Directory to Store the EDA Results
â”œâ”€â”€ src/                                      # Source code
â”‚   â”œâ”€â”€ data_preprocessor/                    # Data Preprocessor Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py  
â”‚   â”‚   â””â”€â”€ data_preprocessor.py              # Python file process the raw data                                       
â”‚   â”œâ”€â”€ exploratory_data_analysis/            # EDA Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py   
â”‚   â”‚   â””â”€â”€ exploratory_data_analyzer.py      # Python file to perform EDA                                   
â”‚   â”œâ”€â”€ normalizer/                           # Text Normalizing Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py                           
â”‚   â”‚   â””â”€â”€ nmf_normalizer.py                 # Python File to Normalize the Preprocessed Data                                    
â”‚   â”œâ”€â”€ recommendation_engine/                # Recommendation Engine Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py   
â”‚   â”‚   â””â”€â”€ similarity_finder.py              # Python file to perform similarity search   
â”‚   â”œâ”€â”€ vectorizer/                           # Recommendation Engine Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py   
â”‚   â”‚   â””â”€â”€ tfidf_vectorizer.py               # Python file to perform vectorizer                             
â”‚   â””â”€â”€ utils/                                # Utility Functions Directory
â”‚       â”œâ”€â”€ __init__.py                     
â”‚       â”œâ”€â”€ data_loader.py                    # Load and Save Data from Local
â”‚       â”œâ”€â”€ download_dataset.py               # Download the Data from Kaggle
â”‚       â””â”€â”€ save_plot.py                      # Save the Plot in Specified Path
â””â”€â”€ web/
    â”œâ”€â”€ __init__.py  
    â”œâ”€â”€ static/                                
    â”‚   â”œâ”€â”€ styles.css                        # Styling of the Web Page
    â”‚   â””â”€â”€ script.js                         # JavaScript File
    â”œâ”€â”€ templates/                                
    â”‚   â””â”€â”€ index.html                        # Default Web Page
    â””â”€â”€ app.py/                               # To run the flask server
        
```

---

## ğŸ”® Future Work Roadmap

The *Inside-Medium* project can be extended significantly to offer a more personalized and intelligent content recommendation system. Here's a proposed roadmap structured in **three development phases**, each with an estimated time frame.

---

### ğŸš€ **Phase 1: UI & API Integration (1â€“2 Weeks)**

**Objective:** Transform the backend logic into a user-accessible application.

*  Build a clean and responsive frontend using **HTML/CSS/JS** for user interaction.
*  Deploy the article recommender as a **Flask API**, allowing input of article titles and displaying similar content.
*  Enable users to upload custom datasets (CSV) for analysis and recommendations.
*  Add search bar, loading indicators, and user-friendly error messages.

### ğŸ§  **Phase 2: Personalization & Topic Modeling (2â€“3 Weeks)**

**Objective:** Enhance the intelligence of the recommender.

*  Introduce **user profiles** to track reading history and provide personalized recommendations.
*  Apply **LDA or BERTopic** for better topic clustering and diversity in suggestions.
*  Integrate **claps, reading time, and tags** more deeply into the similarity scoring system.
*  Include feedback mechanism to rate recommended articles.

### ğŸ§  **Phase 3: Embedding Models & LLM Integration (3â€“4 Weeks)**

**Objective:** Upgrade the recommendation engine with deep learning and language models.

*  Replace TF-IDF + NMF with **sentence embeddings** using `SentenceTransformers` or Hugging Face models.
*  Use **vector databases (e.g., Qdrant, FAISS)** for faster and smarter similarity search.
*  Integrate with **LLMs (e.g., OpenAI, LLaMA via LangChain)** to enable query-based article retrieval using natural language.
*  Package the app into a **Docker container** and deploy to the cloud for scalability.

---


## ğŸ“œ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for more details.

<div align="center">

**Made by Priyam Pal**

[â†‘ Back to Top]

</div>